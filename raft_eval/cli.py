import typer
from raft_eval.evaluation_runner.evaluator import Evaluator
from raft_eval.data_utils.loaders import load_dataset
from raft_eval.logging_utils.logger import save_results
from raft_eval.utils.llm_client import LLMClient
from raft_eval.utils.knowledge_base import KnowledgeBaseRetriever

app = typer.Typer()

@app.command()
def evaluate(
    input_path: str = typer.Option("DataBase.xlsx", help="Path to input file."),
    output_path: str = typer.Option("output.xlsx", help="Path to save output."),
    metrics: str = typer.Option("all", help="Comma-separated list of metrics to evaluate."),
):
    typer.echo(f"üöÄ Starting batch evaluation on {input_path}...")
    
    # Load data
    questions, contexts, answers, gold_answers = load_dataset(input_path)

    
    # Initialize evaluator
    evaluator = Evaluator(metrics=metrics.split(",") if metrics != "all" else "all")
    
    # Run evaluation
    results = evaluator.evaluate_batch(questions, contexts, answers, gold_answers)
    
    # Save results
    save_results(results, output_path)
    
    typer.echo(f"‚úÖ Evaluation completed! Results saved to {output_path}.")

@app.command()
def interactive(
    metrics: str = typer.Option("all", help="Comma-separated list of metrics or 'all'"),
    llm_model: str = typer.Option("deepseek/deepseek-chat-v3-0324", help="LLM model name"),
    knowledge_base_path: str = typer.Option("DataBase.xlsx", help="Path to knowledge base Excel file")
):
    typer.echo("ü§ñ Interactive RAG evaluation mode (Ctrl+C to exit)")

    llm = LLMClient(model_name=llm_model)
    retriever = KnowledgeBaseRetriever(knowledge_base_path)
    evaluator = Evaluator(metrics=metrics.split(",") if metrics != "all" else "all")

    PROMPT_TEMPLATE = (
        "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω–æ–≥–æ –Ω–∏–∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.\n"
        "–ï—Å–ª–∏ –Ω—É–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ù–ï–¢ ‚Äî –æ—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ —Ñ—Ä–∞–∑–æ–π: \"–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\".\n\n"
        "–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n"
        "–í–æ–ø—Ä–æ—Å:\n{question}\n\n"
        "–û—Ç–≤–µ—Ç:"
    )

    while True:
        try:
            question = typer.prompt("Question")
            context = retriever.get_relevant_context(question)
            if context is None:
                context = "–Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
            prompt = PROMPT_TEMPLATE.format(context=context, question=question)

            answer = llm.generate_answer(prompt)

            result = evaluator.evaluate_single(question, context, answer)

            typer.echo("\n--- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã ---")
            typer.echo(f"Question: {question}")
            typer.echo(f"Context: {context}")
            typer.echo(f"LLM Answer: {answer}")
            typer.echo("Metrics:")
            for metric_name, score in result.items():
                typer.echo(f"  {metric_name}: {score}")
            typer.echo("-----------------\n")

        except KeyboardInterrupt:
            typer.echo("\nüëã Exiting interactive RAG mode.")
            break

if __name__ == "__main__":
    app()
